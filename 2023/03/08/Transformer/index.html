

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/logo.svg">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Xander Xu">
  <meta name="keywords" content="">
  
    <meta name="description" content="如果我们把深度学习比作一场革命，那么 Transformer 就是这场革命中的关键转折点。它不仅改变了我们处理语言的方式，还影响了从视觉到音频的各种 AI 应用。但它的本质是什么？让我们一步步揭开这个谜团。">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer 的理解">
<meta property="og:url" content="http://xcq.ink/2023/03/08/Transformer/index.html">
<meta property="og:site_name" content="人菜瘾大的小强">
<meta property="og:description" content="如果我们把深度学习比作一场革命，那么 Transformer 就是这场革命中的关键转折点。它不仅改变了我们处理语言的方式，还影响了从视觉到音频的各种 AI 应用。但它的本质是什么？让我们一步步揭开这个谜团。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://xcq.ink/img/transformer.png">
<meta property="article:published_time" content="2023-03-08T10:00:00.000Z">
<meta property="article:modified_time" content="2025-03-07T04:13:58.054Z">
<meta property="article:author" content="Xander Xu">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="注意力机制">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://xcq.ink/img/transformer.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Transformer 的理解 - 人菜瘾大的小强</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"xcq.ink","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":"ac2559fe71d82eda0e6372616e40089e","google":"G-L228BYNZ7T","tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>人菜瘾大的小强</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Transformer 的理解"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-03-08 18:00" pubdate>
          2023年3月8日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          2.5k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          21 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Transformer 的理解</h1>
            
            
              <div class="markdown-body">
                
                <p>Transformer 是一种基于注意力机制的神经网络架构，它通过并行处理和全局关联让每个元素都能直接”看到”序列中的其他元素，从而高效地捕获长距离依赖关系，彻底改变了自然语言处理、计算机视觉等多个人工智能领域。</p>
<img src="/img/transformer.png" srcset="/img/loading.gif" lazyload alt="transformer结构" width="60%">

<h2 id="为什么需要-Transformer？"><a href="#为什么需要-Transformer？" class="headerlink" title="为什么需要 Transformer？"></a>为什么需要 Transformer？</h2><p>在 Transformer 出现之前，我们处理序列数据（如文本）主要依赖于 RNN（循环神经网络）和 LSTM。但这些模型有一个根本性的限制：它们必须<strong>按顺序处理</strong>信息。</p>



<svg width="600" height="250" viewBox="0 0 600 250">
  <!-- RNN 顺序处理 -->
  <rect x="50" y="50" width="500" height="70" fill="#eee" rx="10" ry="10" stroke="#ccc" />
  <text x="300" y="35" text-anchor="middle" font-size="16" fill="#333">RNN 顺序处理</text>
  
  <rect x="80" y="70" width="60" height="30" fill="#fff" stroke="#333" rx="5" ry="5" />
  <text x="110" y="90" text-anchor="middle" font-size="12">步骤 1</text>
  
  <rect x="190" y="70" width="60" height="30" fill="#fff" stroke="#333" rx="5" ry="5" />
  <text x="220" y="90" text-anchor="middle" font-size="12">步骤 2</text>
  
  <rect x="300" y="70" width="60" height="30" fill="#fff" stroke="#333" rx="5" ry="5" />
  <text x="330" y="90" text-anchor="middle" font-size="12">步骤 3</text>
  
  <rect x="410" y="70" width="60" height="30" fill="#fff" stroke="#333" rx="5" ry="5" />
  <text x="440" y="90" text-anchor="middle" font-size="12">步骤 4</text>
  
  <path d="M140 85 L190 85" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)" />
  <path d="M250 85 L300 85" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)" />
  <path d="M360 85 L410 85" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)" />
  
  <!-- Transformer 并行处理 -->
  <rect x="50" y="150" width="500" height="70" fill="#f0f8ff" rx="10" ry="10" stroke="#ccc" />
  <text x="300" y="135" text-anchor="middle" font-size="16" fill="#333">Transformer 并行处理</text>
  
  <rect x="80" y="170" width="60" height="30" fill="#fff" stroke="#333" rx="5" ry="5" />
  <text x="110" y="190" text-anchor="middle" font-size="12">所有步骤</text>
  
  <rect x="190" y="170" width="60" height="30" fill="#fff" stroke="#333" rx="5" ry="5" />
  <text x="220" y="190" text-anchor="middle" font-size="12">一起</text>
  
  <rect x="300" y="170" width="60" height="30" fill="#fff" stroke="#333" rx="5" ry="5" />
  <text x="330" y="190" text-anchor="middle" font-size="12">同时</text>
  
  <rect x="410" y="170" width="60" height="30" fill="#fff" stroke="#333" rx="5" ry="5" />
  <text x="440" y="190" text-anchor="middle" font-size="12">处理</text>
  
  <line x1="110" y1="200" x2="110" y2="230" stroke="#333" stroke-width="2" />
  <line x1="220" y1="200" x2="220" y2="230" stroke="#333" stroke-width="2" />
  <line x1="330" y1="200" x2="330" y2="230" stroke="#333" stroke-width="2" />
  <line x1="440" y1="200" x2="440" y2="230" stroke="#333" stroke-width="2" />
  
  <!-- 箭头定义 -->
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#333" />
    </marker>
  </defs>
</svg>


<p>这种顺序处理有两个主要问题：</p>
<ol>
<li><strong>处理长序列时效率低下</strong>（想象一下一个词一个词地读完《战争与和平》）</li>
<li><strong>难以捕获远距离的依赖关系</strong>（句子开头和结尾的关联容易被”遗忘”）</li>
</ol>
<blockquote>
<p><strong>核心洞见</strong>：Transformer 的核心创新在于打破了顺序处理的限制，实现了并行计算和全局关联。</p>
</blockquote>
<h2 id="Transformer-的核心：注意力机制"><a href="#Transformer-的核心：注意力机制" class="headerlink" title="Transformer 的核心：注意力机制"></a>Transformer 的核心：注意力机制</h2><p>如果说 Transformer 是一座桥梁，那么注意力机制就是这座桥的基石。它让模型能够”关注”输入中的不同部分，就像我们阅读时会重点关注某些关键词一样。</p>


<svg width="600" height="300" viewBox="0 0 600 300">
  <!-- 输入词语 -->
  <rect x="50" y="50" width="500" height="50" fill="#eee" rx="8" ry="8" />
  
  <rect x="70" y="60" width="80" height="30" fill="#fff" stroke="#333" rx="5" ry="5" />
  <text x="110" y="80" text-anchor="middle" font-size="14">"我"</text>
  
  <rect x="170" y="60" width="80" height="30" fill="#fff" stroke="#333" rx="5" ry="5" />
  <text x="210" y="80" text-anchor="middle" font-size="14">"喜欢"</text>
  
  <rect x="270" y="60" width="80" height="30" fill="#fff" stroke="#333" rx="5" ry="5" />
  <text x="310" y="80" text-anchor="middle" font-size="14">"深度"</text>
  
  <rect x="370" y="60" width="80" height="30" fill="#fff" stroke="#333" rx="5" ry="5" />
  <text x="410" y="80" text-anchor="middle" font-size="14">"学习"</text>
  
  <rect x="470" y="60" width="60" height="30" fill="#fff" stroke="#333" rx="5" ry="5" />
  <text x="500" y="80" text-anchor="middle" font-size="14">"。"</text>
  
  <!-- 注意力权重 -->
  <path d="M310 90 C310 140 110 140 110 90" fill="none" stroke="#D8BFD8" stroke-width="2" />
  <text x="210" y="130" text-anchor="middle" font-size="10" fill="#663399">10%</text>
  
  <path d="M310 90 C310 160 210 160 210 90" fill="none" stroke="#BA55D3" stroke-width="3" />
  <text x="260" y="150" text-anchor="middle" font-size="10" fill="#663399">20%</text>
  
  <path d="M310 90 C310 180 410 180 410 90" fill="none" stroke="#800080" stroke-width="5" />
  <text x="360" y="170" text-anchor="middle" font-size="10" fill="#663399">60%</text>
  
  <path d="M310 90 C310 120 500 120 500 90" fill="none" stroke="#D8BFD8" stroke-width="2" />
  <text x="405" y="110" text-anchor="middle" font-size="10" fill="#663399">10%</text>
  
  <!-- 输出表示 -->
  <rect x="270" y="200" width="80" height="30" fill="#E6E6FA" stroke="#800080" stroke-width="2" rx="5" ry="5" />
  <text x="310" y="220" text-anchor="middle" font-size="14">"深度"</text>
  
<p>  <text x="310" y="250" text-anchor="middle" font-size="12" fill="#555">更新后的表示</text><br>  <text x="310" y="270" text-anchor="middle" font-size="12" fill="#555">（融合了与”学习”的强关联）</text><br></svg></p>
<p>注意力机制的工作原理可以简化为三个步骤：</p>
<ol>
<li><strong>查询（Query）</strong>：当前我们关注的词，比如”深度”</li>
<li><strong>计算相关性</strong>：衡量”深度”与其他每个词的关联程度</li>
<li><strong>加权融合</strong>：根据关联程度，将其他词的信息融入到”深度”的表示中</li>
</ol>
<p>在上图中，当模型处理”深度”这个词时，它会重点关注与之最相关的”学习”，形成”深度学习”的语义关联。</p>
<blockquote>
<p><strong>核心洞见</strong>：自注意力（Self-Attention）是 Transformer 的精髓：每个词都能与序列中的所有词建立直接联系，不受位置远近的限制。</p>
</blockquote>
<h2 id="多头注意力：多角度观察"><a href="#多头注意力：多角度观察" class="headerlink" title="多头注意力：多角度观察"></a>多头注意力：多角度观察</h2><p>如果说单个注意力机制是从一个角度看问题，那么多头注意力（Multi-head Attention）就是同时从多个角度观察。这就像我们理解一部电影时，会同时关注情节、对白、表演和音乐等多个方面。</p>



<svg width="600" height="300" viewBox="0 0 600 300">
  <!-- 输入 -->
  <rect x="50" y="30" width="100" height="240" fill="#eee" rx="8" ry="8" />
  <text x="100" y="150" text-anchor="middle" font-size="16" fill="#333" transform="rotate(-90, 100, 150)">输入序列</text>
  
  <!-- 注意力头 -->
  <rect x="200" y="30" width="80" height="50" fill="#FFD700" stroke="#333" rx="5" ry="5" />
  <text x="240" y="60" text-anchor="middle" font-size="14">头 1</text>
  <text x="240" y="75" text-anchor="middle" font-size="10">语法关系</text>
  
  <rect x="200" y="100" width="80" height="50" fill="#FF6347" stroke="#333" rx="5" ry="5" />
  <text x="240" y="130" text-anchor="middle" font-size="14">头 2</text>
  <text x="240" y="145" text-anchor="middle" font-size="10">语义关系</text>
  
  <rect x="200" y="170" width="80" height="50" fill="#4682B4" stroke="#333" rx="5" ry="5" />
  <text x="240" y="200" text-anchor="middle" font-size="14">头 3</text>
  <text x="240" y="215" text-anchor="middle" font-size="10">上下文关系</text>
  
  <rect x="200" y="240" width="80" height="50" fill="#32CD32" stroke="#333" rx="5" ry="5" />
  <text x="240" y="270" text-anchor="middle" font-size="14">头 4</text>
  <text x="240" y="285" text-anchor="middle" font-size="10">实体关系</text>
  
  <!-- 连接线 -->
  <path d="M150 150 L200 55" stroke="#333" stroke-width="1.5" />
  <path d="M150 150 L200 125" stroke="#333" stroke-width="1.5" />
  <path d="M150 150 L200 195" stroke="#333" stroke-width="1.5" />
  <path d="M150 150 L200 265" stroke="#333" stroke-width="1.5" />
  
  <!-- 输出 -->
  <rect x="330" y="30" width="80" height="240" fill="#f0f8ff" stroke="#333" rx="8" ry="8" />
  <text x="370" y="150" text-anchor="middle" font-size="16" fill="#333" transform="rotate(-90, 370, 150)">多视角表示</text>
  
  <path d="M280 55 L330 80" stroke="#FFD700" stroke-width="2" />
  <path d="M280 125 L330 120" stroke="#FF6347" stroke-width="2" />
  <path d="M280 195 L330 160" stroke="#4682B4" stroke-width="2" />
  <path d="M280 265 L330 200" stroke="#32CD32" stroke-width="2" />
  
  <!-- 整合 -->
  <rect x="450" y="110" width="100" height="80" fill="#E6E6FA" stroke="#333" rx="8" ry="8" />
  <text x="500" y="150" text-anchor="middle" font-size="16" fill="#333">整合结果</text>
  <text x="500" y="170" text-anchor="middle" font-size="12" fill="#555">深度理解</text>
  
  <path d="M410 150 L450 150" stroke="#333" stroke-width="2" marker-end="url(#arrowhead2)" />
  
  <!-- 箭头定义 -->
  <defs>
    <marker id="arrowhead2" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#333" />
    </marker>
  </defs>
</svg>


<p>多头注意力的好处：</p>
<ol>
<li><strong>增强表达能力</strong>：可以捕捉不同类型的语言关系（语法、语义、主题等）</li>
<li><strong>提供稳定梯度</strong>：改善训练过程，让模型收敛更快更稳定</li>
<li><strong>增加鲁棒性</strong>：减少了对单一特征的依赖，使模型更加健壮</li>
</ol>
<p>多头注意力机制让 Transformer 能够同时从多个角度理解输入，类似于人类使用多种感官和认知角度来理解世界。</p>
<h2 id="Transformer-的结构：编码器与解码器"><a href="#Transformer-的结构：编码器与解码器" class="headerlink" title="Transformer 的结构：编码器与解码器"></a>Transformer 的结构：编码器与解码器</h2><p>完整的 Transformer 由编码器（Encoder）和解码器（Decoder）组成，但很多现代应用（如 BERT）只使用编码器，而像 GPT 这样的模型则主要基于解码器。</p>



<svg width="600" height="400" viewBox="0 0 600 400">
  <!-- 编码器 -->
  <rect x="100" y="50" width="160" height="300" fill="#eee" rx="8" ry="8" stroke="#ccc" />
  <text x="180" y="30" text-anchor="middle" font-size="16" fill="#333">编码器</text>
  
  <!-- 多头注意力 -->
  <rect x="120" y="70" width="120" height="60" fill="#f0f8ff" stroke="#333" rx="5" ry="5" />
  <text x="180" y="100" text-anchor="middle" font-size="14">多头自注意力</text>
  <text x="180" y="115" text-anchor="middle" font-size="10">理解输入的关系</text>
  
  <!-- 前馈网络 -->
  <rect x="120" y="150" width="120" height="60" fill="#E6E6FA" stroke="#333" rx="5" ry="5" />
  <text x="180" y="180" text-anchor="middle" font-size="14">前馈神经网络</text>
  <text x="180" y="195" text-anchor="middle" font-size="10">进一步处理特征</text>
  
  <!-- 规范化 -->
  <rect x="120" y="230" width="120" height="60" fill="#F0FFF0" stroke="#333" rx="5" ry="5" />
  <text x="180" y="260" text-anchor="middle" font-size="14">层规范化</text>
  <text x="180" y="275" text-anchor="middle" font-size="10">稳定训练过程</text>
  
  <!-- 连接线 -->
  <path d="M180 130 L180 150" stroke="#333" stroke-width="1.5" marker-end="url(#arrowhead3)" />
  <path d="M180 210 L180 230" stroke="#333" stroke-width="1.5" marker-end="url(#arrowhead3)" />
  
  <!-- 解码器 -->
  <rect x="340" y="50" width="160" height="300" fill="#f5f5f5" rx="8" ry="8" stroke="#ccc" />
  <text x="420" y="30" text-anchor="middle" font-size="16" fill="#333">解码器</text>
  
  <!-- 多头注意力 -->
  <rect x="360" y="70" width="120" height="60" fill="#f0f8ff" stroke="#333" rx="5" ry="5" />
  <text x="420" y="100" text-anchor="middle" font-size="14">多头自注意力</text>
  <text x="420" y="115" text-anchor="middle" font-size="10">理解已生成的内容</text>
  
  <!-- 交叉注意力 -->
  <rect x="360" y="150" width="120" height="60" fill="#FFE4E1" stroke="#333" rx="5" ry="5" />
  <text x="420" y="180" text-anchor="middle" font-size="14">交叉注意力</text>
  <text x="420" y="195" text-anchor="middle" font-size="10">关联编码器的输出</text>
  
  <!-- 前馈网络 -->
  <rect x="360" y="230" width="120" height="60" fill="#E6E6FA" stroke="#333" rx="5" ry="5" />
  <text x="420" y="260" text-anchor="middle" font-size="14">前馈神经网络</text>
  <text x="420" y="275" text-anchor="middle" font-size="10">生成最终输出</text>
  
  <!-- 连接线 -->
  <path d="M420 130 L420 150" stroke="#333" stroke-width="1.5" marker-end="url(#arrowhead3)" />
  <path d="M420 210 L420 230" stroke="#333" stroke-width="1.5" marker-end="url(#arrowhead3)" />
  
  <!-- 编码器到解码器 -->
  <path d="M260 175 L360 175" stroke="#333" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowhead3)" />
  <text x="310" y="160" text-anchor="middle" font-size="12" fill="#555">信息传递</text>
  
  <!-- 输入和输出 -->
  <rect x="100" y="370" width="160" height="30" fill="#FFF0F5" stroke="#333" rx="5" ry="5" />
  <text x="180" y="390" text-anchor="middle" font-size="14">输入序列</text>
  
  <rect x="340" y="370" width="160" height="30" fill="#F0FFF0" stroke="#333" rx="5" ry="5" />
  <text x="420" y="390" text-anchor="middle" font-size="14">输出序列</text>
  
  <path d="M180 50 L180 30 L180 10 L180 370" stroke="#FF6347" stroke-width="1.5" stroke-dasharray="3,3" />
  <path d="M420 50 L420 30 L420 10 L420 370" stroke="#4682B4" stroke-width="1.5" stroke-dasharray="3,3" />
  
  <!-- 箭头定义 -->
  <defs>
    <marker id="arrowhead3" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#333" />
    </marker>
  </defs>
</svg>


<p>Transformer 的魔力不仅来自于注意力机制，还有一些关键的技术细节：</p>
<ol>
<li><strong>位置编码</strong>：由于注意力机制本身不包含位置信息，Transformer 使用特殊的位置编码来告诉模型每个词的位置</li>
<li><strong>残差连接</strong>：帮助信息在网络中更顺畅地流动，避免梯度消失问题</li>
<li><strong>层规范化</strong>：稳定训练过程，加速收敛</li>
</ol>
<h3 id="编码器与解码器的区别"><a href="#编码器与解码器的区别" class="headerlink" title="编码器与解码器的区别"></a>编码器与解码器的区别</h3><ul>
<li><strong>编码器</strong>：专注于理解输入序列，捕捉上下文信息</li>
<li><strong>解码器</strong>：专注于生成输出序列，同时关注已生成的内容和编码器提供的上下文</li>
</ul>
<p>最关键的是两者之间的桥梁：<strong>交叉注意力</strong>机制，它使解码器能够”查询”编码器获取相关信息，从而生成更准确的输出。</p>
<h2 id="Transformer-的数学原理简述"><a href="#Transformer-的数学原理简述" class="headerlink" title="Transformer 的数学原理简述"></a>Transformer 的数学原理简述</h2><p>虽然 Transformer 的核心思想很直观，但它的实现涉及一些数学运算。以自注意力为例：</p>
<ol>
<li>每个输入词被表示为一个向量</li>
<li>每个词生成三个向量：查询（Q）、键（K）和值（V）</li>
<li>注意力权重的计算公式为：</li>
</ol>
<p>$$<br>\text{Attention}(Q, K, V) &#x3D; \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V<br>$$</p>
<p>这个公式看起来复杂，但本质上就是：</p>
<ul>
<li>计算查询与所有键的相似度（QK^T）</li>
<li>进行缩放（除以 √d_k）防止梯度消失</li>
<li>使用 softmax 将相似度转换为权重</li>
<li>用这些权重对值向量进行加权求和</li>
</ul>
<h2 id="Transformer-的应用：无处不在"><a href="#Transformer-的应用：无处不在" class="headerlink" title="Transformer 的应用：无处不在"></a>Transformer 的应用：无处不在</h2><p>自 2017 年提出以来，Transformer 已经成为 AI 领域的基础架构，其应用远超最初的机器翻译任务：</p>



<svg width="600" height="250" viewBox="0 0 600 250">
  <!-- 中心 -->
  <circle cx="300" cy="125" r="60" fill="#f0f8ff" stroke="#333" stroke-width="2" />
  <text x="300" y="130" text-anchor="middle" font-size="16" font-weight="bold">Transformer</text>
  
  <!-- 应用 -->
  <circle cx="150" cy="75" r="45" fill="#FFE4E1" stroke="#333" />
  <text x="150" y="75" text-anchor="middle" font-size="14">BERT</text>
  <text x="150" y="90" text-anchor="middle" font-size="10">文本理解</text>
  
  <circle cx="150" cy="175" r="45" fill="#E6E6FA" stroke="#333" />
  <text x="150" y="175" text-anchor="middle" font-size="14">GPT</text>
  <text x="150" y="190" text-anchor="middle" font-size="10">文本生成</text>
  
  <circle cx="450" cy="75" r="45" fill="#F0FFF0" stroke="#333" />
  <text x="450" y="75" text-anchor="middle" font-size="14">ViT</text>
  <text x="450" y="90" text-anchor="middle" font-size="10">视觉理解</text>
  
  <circle cx="450" cy="175" r="45" fill="#F5F5DC" stroke="#333" />
  <text x="450" y="175" text-anchor="middle" font-size="14">Wav2Vec</text>
  <text x="450" y="190" text-anchor="middle" font-size="10">语音识别</text>
  
  <!-- 连接线 -->
  <line x1="250" y1="105" x2="180" y2="75" stroke="#333" stroke-width="1.5" />
  <line x1="250" y1="145" x2="180" y2="175" stroke="#333" stroke-width="1.5" />
  <line x1="350" y1="105" x2="420" y2="75" stroke="#333" stroke-width="1.5" />
  <line x1="350" y1="145" x2="420" y2="175" stroke="#333" stroke-width="1.5" />
</svg>


<p>Transformer 派生出的几个重要模型家族：</p>
<ol>
<li><strong>BERT</strong>（由 Google 开发）：专注于理解文本，通过预训练学习双向上下文</li>
<li><strong>GPT</strong>（由 OpenAI 开发）：专注于生成文本，每次预测下一个词</li>
<li><strong>ViT</strong>（Vision Transformer）：将图像分解为”视觉词元”，用 Transformer 处理图像</li>
<li><strong>Wav2Vec</strong>：将语音信号转换为离散表示，再用 Transformer 进行处理</li>
</ol>
<blockquote>
<p><strong>核心洞见</strong>：Transformer 之所以能取得如此广泛的成功，是因为它提供了一种通用的方法来处理不同类型的序列数据，无论是文本、图像还是音频。</p>
</blockquote>
<h2 id="从原始-Transformer-到现代变体"><a href="#从原始-Transformer-到现代变体" class="headerlink" title="从原始 Transformer 到现代变体"></a>从原始 Transformer 到现代变体</h2><p>原始的 Transformer 模型虽然强大，但仍有一些限制。随着研究的深入，Transformer 架构不断演化：</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>年份</th>
<th>主要改进</th>
</tr>
</thead>
<tbody><tr>
<td>原始 Transformer</td>
<td>2017</td>
<td>提出注意力机制和编码器-解码器架构</td>
</tr>
<tr>
<td>BERT</td>
<td>2018</td>
<td>双向编码，掩码语言模型预训练</td>
</tr>
<tr>
<td>GPT-2</td>
<td>2019</td>
<td>更大模型，零样本学习能力</td>
</tr>
<tr>
<td>T5</td>
<td>2020</td>
<td>将所有 NLP 任务统一为文本到文本的格式</td>
</tr>
<tr>
<td>GPT-3</td>
<td>2020</td>
<td>极大规模参数，少样本学习能力</td>
</tr>
<tr>
<td>ViT</td>
<td>2020</td>
<td>将 Transformer 应用于视觉任务</td>
</tr>
<tr>
<td>CLIP</td>
<td>2021</td>
<td>将文本和图像表示在同一空间</td>
</tr>
<tr>
<td>DALL-E</td>
<td>2021</td>
<td>从文本描述生成图像</td>
</tr>
<tr>
<td>PaLM&#x2F;GPT-4</td>
<td>2022&#x2F;2023</td>
<td>更大规模，多模态，推理能力提升</td>
</tr>
</tbody></table>
<p>这种演化表明，Transformer 架构具有惊人的适应性和可扩展性，能够应对各种 AI 挑战。</p>
<h2 id="Transformer-的局限性"><a href="#Transformer-的局限性" class="headerlink" title="Transformer 的局限性"></a>Transformer 的局限性</h2><p>尽管 Transformer 革命性地改变了 AI 领域，但它也有一些局限性：</p>
<ol>
<li><strong>计算复杂度</strong>：标准的自注意力机制的复杂度是 O(n²)，其中 n 是序列长度，这使得处理长序列成为挑战</li>
<li><strong>位置编码的局限</strong>：传统的位置编码无法很好地处理超出训练范围的更长序列</li>
<li><strong>训练成本高</strong>：预训练大型 Transformer 模型需要大量计算资源</li>
<li><strong>解释性差</strong>：虽然注意力权重提供了一些可视化，但模型决策过程整体上仍然是个黑盒</li>
</ol>
<p>为了解决这些问题，研究人员提出了许多改进方案：</p>
<ul>
<li><strong>稀疏注意力机制</strong>：如 Reformer、Longformer 等，通过只关注部分位置降低计算复杂度</li>
<li><strong>线性注意力</strong>：如 Performer、Linear Transformer 等，将复杂度从 O(n²) 降至 O(n)</li>
<li><strong>参数共享</strong>：如 Albert，通过跨层参数共享减少模型大小</li>
<li><strong>知识蒸馏</strong>：从大模型中提取知识到小模型，提高效率</li>
</ul>
<h2 id="Transformer-的本质总结"><a href="#Transformer-的本质总结" class="headerlink" title="Transformer 的本质总结"></a>Transformer 的本质总结</h2><p>经过上面的探索，我们可以提炼出 Transformer 架构的三个本质特性：</p>



<svg width="600" height="200" viewBox="0 0 600 200">
  <!-- 背景 -->
  <rect width="600" height="200" fill="#f9f9f9" rx="10" ry="10" />
  
  <!-- 三个核心元素 -->
  <circle cx="150" cy="100" r="60" fill="#e6f7ff" stroke="#1890ff" stroke-width="2" />
  <text x="150" y="90" text-anchor="middle" font-size="16" font-weight="bold" fill="#1890ff">注意力机制</text>
  <text x="150" y="110" text-anchor="middle" font-size="12" fill="#333">关注重要信息</text>
  
  <circle cx="300" cy="100" r="60" fill="#f6ffed" stroke="#52c41a" stroke-width="2" />
  <text x="300" y="90" text-anchor="middle" font-size="16" font-weight="bold" fill="#52c41a">并行处理</text>
  <text x="300" y="110" text-anchor="middle" font-size="12" fill="#333">高效计算</text>
  
  <circle cx="450" cy="100" r="60" fill="#fff2e8" stroke="#fa8c16" stroke-width="2" />
  <text x="450" y="90" text-anchor="middle" font-size="16" font-weight="bold" fill="#fa8c16">全局视野</text>
  <text x="450" y="110" text-anchor="middle" font-size="12" fill="#333">长距离依赖</text>
  
  <!-- 连接线 -->
  <path d="M205 80 L245 80" stroke="#333" stroke-width="1.5" />
  <path d="M205 120 L245 120" stroke="#333" stroke-width="1.5" />
  <path d="M355 80 L395 80" stroke="#333" stroke-width="1.5" />
  <path d="M355 120 L395 120" stroke="#333" stroke-width="1.5" />
  
  <!-- 标题 -->
<p>  <text x="300" y="30" text-anchor="middle" font-size="20" font-weight="bold" fill="#333">Transformer 的本质</text><br></svg></p>
<ol>
<li><strong>注意力机制</strong>：让模型能够选择性地关注输入中的重要部分，类似于人类的注意力焦点</li>
<li><strong>并行处理</strong>：打破顺序处理的限制，实现高效计算，大幅提升训练和推理速度</li>
<li><strong>全局视野</strong>：每个位置都能直接”看到”序列中的任何其他位置，解决长距离依赖问题</li>
</ol>
<p>这三个核心特性共同作用，使 Transformer 成为处理序列数据的强大工具，无论是自然语言、代码、图像、音频还是视频。</p>
<h2 id="展望未来"><a href="#展望未来" class="headerlink" title="展望未来"></a>展望未来</h2><p>Transformer 的出现彻底改变了 AI 的发展轨迹。展望未来，我们可以预见几个发展方向：</p>
<ol>
<li><strong>更高效的 Transformer 变体</strong>：降低计算复杂度，处理更长序列</li>
<li><strong>多模态 Transformer</strong>：统一处理文本、图像、音频和视频</li>
<li><strong>领域特定的 Transformer</strong>：针对科学计算、医疗、金融等特定领域优化</li>
<li><strong>更具可解释性的 Transformer</strong>：让模型决策过程更加透明</li>
<li><strong>结合神经符号方法</strong>：将 Transformer 的表示能力与符号推理结合</li>
</ol>
<p>无论未来如何发展，Transformer 已经成为 AI 历史上的一个里程碑，它不仅是一种模型架构，更是一种思维方式的转变——从顺序到并行，从局部到全局。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Transformer &#x3D; 注意力机制 + 并行处理 + 全局视野</p>
<p>这个简单而强大的公式，重塑了人工智能的面貌，并将继续影响其未来发展。理解 Transformer 的本质，就是理解现代 AI 的基础。</p>
<hr>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Vaswani, A., et al. (2017). “Attention is All You Need”. NeurIPS 2017.</li>
<li>Devlin, J., et al. (2018). “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. NAACL 2019.</li>
<li>Brown, T., et al. (2020). “Language Models are Few-Shot Learners”. NeurIPS 2020.</li>
<li>Dosovitskiy, A., et al. (2020). “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”. ICLR 2021.</li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E7%AE%97%E6%B3%95/" class="category-chain-item">算法</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Transformer/" class="print-no-link">#Transformer</a>
      
        <a href="/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" class="print-no-link">#注意力机制</a>
      
        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="print-no-link">#深度学习</a>
      
        <a href="/tags/NLP/" class="print-no-link">#NLP</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Transformer 的理解</div>
      <div>http://xcq.ink/2023/03/08/Transformer/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Xander Xu</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年3月8日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/06/03/SVM%E7%9A%84%E6%9C%AC%E8%B4%A8%EF%BC%9A%E4%BB%8E%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E5%88%B0%E6%95%B0%E5%AD%A6%E6%B7%B1%E5%BA%A6/" title="SVM的本质：从直观理解到数学原理">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">SVM的本质：从直观理解到数学原理</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/01/23/%E4%BA%BA%E4%B8%8EAI%E7%9A%84%E5%85%B3%E7%B3%BB/" title="人与AI的关系:我们如何证明自己不是机器？">
                        <span class="hidden-mobile">人与AI的关系:我们如何证明自己不是机器？</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    
      <script type="text/javascript">
        var disqus_config = function() {
          this.page.url = 'http://xcq.ink/2023/03/08/Transformer/';
          this.page.identifier = '/2023/03/08/Transformer/';
        };
        Fluid.utils.loadComments('#disqus_thread', function() {
          var d = document, s = d.createElement('script');
          s.src = '//' + 'fluid' + '.disqus.com/embed.js';
          s.setAttribute('data-timestamp', new Date());
          (d.head || d.body).appendChild(s);
        });
      </script>
    
    <noscript>Please enable JavaScript to view the comments</noscript>
  </div>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
